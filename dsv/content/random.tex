Bisher waren wir ausschließlich mit deterministischen Signalen befasst. 
Also solchen, welche sich bequem durch eine Funktionsvorschrift $t \mapsto x(t)$ beschreiben lassen.
Doch eigentlich ist dies ein absolut idealisierter Spezialfall, da für gew\"ohnlich alle Signale, die wir verarbeiten, zu mindestens einem Zeitpunkt von einem Messgerät beobachtet wurden.
Somit sind diese notwendigerweise mit Messrauschen behaftet, dessen unangenehme Eigenschaft es ist, zuällig zu sein.
Aus diesem Grund ist es uns nicht m\"oglich, ein gemessenes und digitalisiertes Signal $x[\cdot]$ mittels eines alebraischen Ausdrucks zu beschreiben.
Notwendigerweise wird $x[\cdot]$ zu einer sog. Zufallsgr\"oße, welche wir \q{nur} noch durch deren statistische Eigenschaften beschreiben k\"onnen.
Nicht nur das, auch jede folgende Verarbeitung von $x[\cdot]$, bespielsweise durch ein \gls{lti}-System, wird zu einer Zufallsgr\"oße, deren Eigenschaften u.A. von denen des Messrauschens abhängen.

Weiterhin kann es für gewisse Anwendungen auch absolut ausreichend sein, eigentlich deterministische Signale als zufällig zu modellieren.
Wenn es beispielsweise darum geht, zu bestimmen, ob ein Kommunikationssystem in der Lage ist, Informationen fehlerfrei zu übertragen, dann kann es ausreichend sein, \q{nur} eine Ausfallrate/-wahrscheinlichkeit zu bestimmen, oder diese Rate zu begrenzen, anstatt dies für jedes m\"ogliche zu übertragende Signal zu bestimmen/gewährleisten.

Um diese \q{neue} Sichtweise auf Signale gut verinnerlichen zu k\"onnen, ben\"otigen wir jedoch einige Werkzeuge aus der Wahrscheinlichkeitstheorie, die wir uns nun ansehen werden, wobei wir versuchen, uns nicht mit vielen mathematischen Details aufzuhalten, sondern wollen stattdessen unsere Intuition schärfen.

\subsection{Grundlagen Wahrscheinlichkeitstheorie}\label{sec:random:prbly}

Nehmen wir zunächst einmal an, dass wir einen zufälligen Vorgang betrachten, dessen Ausgang sich in Form einer einzelnen reellen Zahl manifestiert.
Diesen zufälligen Vorgang assoziieren wir mit der Zufallsgr\"oße $X : \Omega \rightarrow \R$, also $\omega \rightarrow X(\omega)$, wobei wir uns vorstellen, dass $\Omega$, beziehungsweise $\omega$, dafür sorgen, dass Zufall ins Spiel kommt. \textit{$\ast$MITHÄNDENWEDEL$\ast$} 
Wir sind nun daran interessiert, wie die \emph{Verteilung} der Werte $X(\omega)$ charakterisiert werden kann.

Hierzu definieren wir beigeordnet zu $X$ eine Funktion $F_X: \R \rightarrow [0,1]$, welche uns für $x \in \R$ mitteilt, was die Wahrscheinlichkeit für das Ereignis $X \leqslant x$ ist.
Man kann auch sagen/schreiben:
\[
F_X(x) = P(X \leqslant x).
\]
Die Funktion $F$ ist monoton wachsend, da natürlich $P(X \leqslant x_1) \leqslant P(X \leqslant x_2)$, falls $x_1 < x_2$, also auch $F_X(x_1) \leqslant F_X(x_2)$.
Jetzt kann man mit der \emph{Verteilungsfunktion} $F_X$ schon eine Menge anfangen.
Beispielsweise k\"onnen wir damit auch bestimmen, mit welcher Wahrscheinlichkeit das Ereignis $x_1 < X \leqslant x_2$ eintritt, indem wir einfach 
\[
P(x_1 < X \leqslant x_2) = P(\{x_1 < X\} \cup \{X < x_2\}) = F_X(x_2) - F_X(x_1)
\]
berechnen.

Wir k\"onnen sogar noch einen Schritt weiter gehen, und die Gr\"oße
\[
f^h_X(x) = \frac{F_X(x + h) - F_X(x)}{h}
\]
betrachten.
Man kann es sich so vorstellen, dass wir das Intervall $[x,x+h]$ hernehmen, berechnen, wie \q{viel} von $X$ in $[x,x+h]$ landen kann, und normieren das mit der Intervallbreite $h$.
Im Grunde berechnen wir, wie \q{dicht} die Zufallsgr\"oße zwischen $x$ und $x+h$ ist.
Aus diesem Grund nennt man den Grenzwert
\[
f_X(x) = \lim\limits_{h \rightarrow 0} f^h_X(x) = \lim\limits_{h \rightarrow 0} \frac{F_X(x + h) - F_X(x)}{h}
\]
die \emph{Dichte} von $X$.
Natürlich sehen wir, dass der berechnete Grenzwert der sogenannte Differenzenquotient der Verteilungsfunktion $F_x$ ist. Es gilt also $f_X = F^\prime_X = (\mathrm d)/{\mathrm d x} F_{x}(x)$.
Deshalb kann man nun die Verteilungsfunktion auch aus der Dichte gewinnen, indem wir
\[
F_X(x) = \Int{-\infty}{x}{f_X(s)}{s}
\]
auswerten.
Die obigen Betrachtungen gelten freilich nur, wenn die formulierten Grenzwerte auch immer existieren, doch erstens existieren diese meistens und für ein intuitives Verständnis sollte es zunächst genügen.

Weiterhin gilt für reelle Zufallsgrößen $X$, dass diese sich \q{irgendwo} zwischen $-\infty$ und $+\infty$ mit Wahrscheinlichkeit $1$ realisieren muss.
Also gilt
\[
\lim_{x \rightarrow \infty} F_X(x) = 1,
\]
was für die Dichte ergibt, dass
\[
\Int{-\infty}{+\infty}{f_X(s)}{s} = 1.
\]
Da $F_X$ monoton wachsend ist, muss auch immer gelten, dass $f_X(x) \geqslant 0$.
%
%
\subsubsection{Unabhängigkeit}\label{sec:random:unabh}
%
%
Gegeben zwei Zufallsgrößen $X$ und $Y$ ist es oft interessant deren Verhältnis zueinander zu untersuchen. 
Beispielsweise wollen wir $P(X \leqslant x, Y \leqslant y)$, also die Wahrscheinlichkeit, dass $X \leqslant x$ und $Y \leqslant y$ gemeinsam (gleichzeitig) eintreten, berechnen/untersuchen/kennen.
Im allgemeinen Fall, benötigt man hierfür die \emph{gemeinsame} Verteilung von $X$ und $Y$, also $F_{X,Y}$ oder $f_{X,Y}$.

Gilt aber für alle $x,y$, dass
\begin{equation}\label{eq:random:unabh}
    P(X \leqslant x, Y \leqslant y) = P(X \leqslant x) \cdot P(Y \leqslant y),
\end{equation}
so nennt man $X$ und $Y$ (stochastisch) \emph{unabhängig}. 
Die gemeinsame Verteilung von $X$ und $Y$ ergibt sich also als das Produkt der Einzelverteilungen, was \emph{sehr} viele Berechnungen vereinfacht.
Jedoch ist es schwer Unabhängigkeit nachzuweisen, weshalb es oft eine Annahme ist, die man im Vorhinein während der Modellierung eines Zufallsprozesses trifft.
%
%
\subsubsection{Diskrete Verteilungen}
%
Bisher haben wir immer den Fall betrachtet, dass die Verteilungsfunktion $F_X$ (nicht die Dichte $f_X$, siehe uniforme Verteilung) immer stetig war.
In diesem Fall spricht man auch von stetigen Verteilungen.
Deren Eigenschaft ist es (intuitiv), dass wenn $f_X(x) > 0$ für ein $x$, dann gibt es auch in jeder Umgebung, egal wie klein, um $x$ noch Punkte $y$ mit $f_X(x) > 0$.
Bei diskreten Verteilungen kann es sein, dass \q{Wahrscheinlichkeitsmasse} auf isolierte (diskrete) \emph{Punkte} konzentriert ist.

Ein einfaches Beispiel (siehe \Cref{sec:random:quanti}) wäre die quantisierte Version $Q(X)$ einer stetigen Zufallsgröße $X$. 
Dann erhalten wir mit $Q(X)$ wieder eine Zufallsgröße, deren Werte aber auf die Quantisierungsstufen $\mathcal{Q}$ beschränkt sind.
Da wir später diese Art von Verteilung irgendwie in den Griff bekommen müssen, wollen wir uns kurz damit befassen.

Eigentlich müssen diskrete Verteilungen kein Spezialfall von kontinuierlichen Verteilungen sein, wenn man die richtige Theorie\footnote{\url{https://en.wikipedia.org/wiki/Measure_(mathematics)}} verwendet.
Um das zu vermeiden, müssen wir auf eine etwas unsaubere Schreib- und Denkweise zurückgreifen und im Hinterkopf behalten, dass hier einige Spitzfindigkeiten versteckt sind.

Hierzu nutzen wir den stetigen Dirac-Stoß.
Gegeben eine Menge von diskreten Punkten der Form $D = \{d_k \vert k \in \Z \}$ und zugehörige Wahrscheinlichkeiten $p_k \geqslant 0$ für $k \in \Z$ mit der Eigenschaft, dass
\[
\Sum{k \in \Z}{}{p_k} = 1,
\]
dann definieren wir eine diskrete Zufallsgröße $X$ mit Dichte $f_X$ durch
\begin{equation}\label{eq:diskrete_dichte}
    f_X(x) = \Sum{k \in \Z}{}{p_k \delta(x - d_k)}.
\end{equation}
Wir heften also an die Punkte in $D$ jeweile einen Dirac-Stoß, der mit der Wahrscheinlichkeit für diesen Punkt gewichtet wird.
Dann ergibt sich also direkt aus den Eigenschaften von $\delta$, dass $P(X = d_k) = p_k$ und $P(X=x) = 0$ für $x \notin D$, siehe \Cref{py:random:discrete}.

\begin{listing}[ht]
    \noindent
    \begin{minipage}{0.51\textwidth}
        \strut\vspace*{-\baselineskip}\newline
        \inputminted[firstline=4, lastline=18]{python3}{code/random/discrete.py}
    \end{minipage}%
    \begin{minipage}{0.48\textwidth}
        \strut\vspace*{-\baselineskip}\newline
        \includegraphics[width=\textwidth]{code/random/discrete.png}
    \end{minipage}
    \codecaption{dsv/code/random/discrete.py}{Visualisierung der Dichte und der Verteilungsfunktion einer diskreten Zufallsgröße}\label{py:random:discrete}
\end{listing}

Mithilfe von $\delta$ sind wir also in der Lage, weiterhin die aus der Signalverarbeitung bekannten Integrale und deren Eigenschaften in bezug auf $\delta$ zu nutzen.
%
%
\subsubsection{Beispiele}
%
Es ist zunächst noch eine Schreibweise einzuführen. 
Wir werden die Notation $X \sim V$, wobei gemeint ist, dass $V$ für eine Verteilung steht, die die Dichte und damit auch die Verteilungsfunktion und damit auch das oben definierte $P$ festlegt. 
Wenn wir nun schreiben $X,Y \sim V$ (oder kurz: $X \sim Y$), dann meint das \emph{nicht}, dass $X = Y$!
Es meint nur, dass $X$ und $Y$ derselben Verteilung folgen, aber nicht, dass deren Realisierungen $X(\omega)$ und $Y(\omega)$ immer diesselben Werte annehmen. 
Das Ereignis $X = Y$ tritt also nur mit einer Wahrscheinlichkeit $P(X = Y)$ ein, wobei wir hierfür Kenntnis über die \emph{gemeinsame Dichte} $f_{X,Y}(x,y)$ benötigen.
Es ist bei vielen unserer Beispiele sehr oft der Fall, dass $P(X = Y) = 0$, wenn $X \sim Y$.
%
\paragraph{Uniforme Verteilung/Gleichverteilung}
Gegeben zwei reelle Zahlen $a < b$ nennt man eine Zufallsgröße $X$ verteilt nach $\Unif(a,b)$, man schreibt $X \sim \Unif(a,b)$, wenn die Dichte $f_X$ gegeben ist durch
\[
f_X(x) = \begin{cases}
    0, \Text{falls} x<a, \\
    1/(b-a), \Text{falls} a \leqslant x < b \\
    0, \Text{falls} b \leqslant x.
\end{cases} = 
    \frac 1{b-a} \Rect\left(\frac{x-(b+a)/2}{b-a}\right)
\]
Der Name dieser Verteilung ergibt sich daraus, dass intuitiv alle Intervalle, die im Intervall $[a,b]$ liegen mit einer Wahrscheinlichkeit proportional zu ihrer \q{Länge} von $X$ \q{getroffen} werden.
Man schreibt auch oft $X \sim \Unif(a,b)$.
%
\paragraph{Normalverteilung}
Gegeben zwei Parameter $\mu \in \R$ und $\sigma^2 \in \R^+$, so nennt man eine Zufallsgröße $X$ normalverteilt mit Erwartungswert $\mu$ und Varianz $\sigma^2$, falls deren Dichte $f_X$
\[
f_X(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2 \sigma^2}\right)
\]
erfüllt.
Die Normalverteilungist eine sehr wichtige Verteilung, weil aus Gründen die meisten Messwerte einer Normalverteilung mit Erwartungswert $\mu$ und einer gewissen Varianz $\sigma^2$ folgen.
Je größer $\sigma^2$, desto stärker ist die Streuung der gemessenen Werte um den Erwartungswert $\mu$.
Man schreibt auch $X \sim \mathcal{N}(\mu, \sigma^2)$.
Es sind zwei Beispiele in \Cref{py:random:normal1} zusammen mit der Berechnung der Dichte via \texttt{scipy} dargestellt.
Auch hier ist es wieder ratsam, keine eigenen Implementierungen solcher Standardfunktionen zu nutzen und stattdessen auf Funktionen aus getesteten Paketen zurück zu greifen.
\begin{listing}[ht]
    \noindent
    \begin{minipage}{0.51\textwidth}
        \strut\vspace*{-\baselineskip}\newline
        \inputminted[firstline=3, lastline=10]{python3}{code/random/normal1.py}
    \end{minipage}%
    \begin{minipage}{0.48\textwidth}
        \strut\vspace*{-\baselineskip}\newline
        \includegraphics[width=\textwidth]{code/random/normal1.png}
    \end{minipage}
    \codecaption{dsv/code/random/normal1.py}{Visualisierung der Dichte der Normalverteilung für verschiedene Parameter}\label{py:random:normal1}
\end{listing}
%
%
\paragraph{Exponentialverteilung}
Gegeben eine Intensität $\lambda > 0$, so folgt eine Zufallsgröße $X$ einer Exponentialverteilung $\Exp(\lambda)$, falls die Dichte $f_X$ die Form 
\[
f_X(x) = \begin{cases}
    \lambda \exp(-\lambda x), \Text{für} x \geqslant
    0, \\
    0 \Text{sonst}
\end{cases}
\]
besitzt.
Auch eine wichige Verteilung, die beispielsweise die Lebensdauer von irgendwelchen Bauteilen modelliert, oder auch die Wartezeit zwischen zwei Anrufen bei einer Servicehotline. 
Je kleiner $\lambda$, desto höher die Intensität, also desto kürzer die Wartezeit.
%
%
\paragraph{Laplace- und Binomialverteilung}
%
Wir stellen uns vor, dass wir ein Zufallsexperiment $X$ durchführen, bei dem Erfolg $X=1$ mit Wahrscheinlichkeit $p$ auftritt und Misserfolg $X=0$ mit Wahrscheinlichkeit $(1-p)$.
Demzufolge gibt es nur diese beiden Möglichkeiten, und wir haben als Dichte entsprechend
\[
f_{X}(x) = (1-p) \delta(x) + p \delta(x-1). 
\]
Solche eine diskrete Verteilung nennt man Laplaceverteilung mit Parameter $p$.

Allein ist diese Verteilung so einfach, wie sie nur sein kann.
Betrachten wir aber nun $n$ viele solcher \emph{unabhängiger} Laplaceexperimente $X_1, \ldots X_n$, man nennt diese dann \gls{iid}, und zählen die Anzahl der Erfolge $X$ in diesen $n$ Experimenten, also 
\[
    X = \Sum{i = 1}{n}{X_n},
\]
so erhalten wir als Dichte
\[
    f_{X}(x) = \Sum{k = 0}{n}{\binom{n}{k} p^k (1-p)^{n-k} \delta(x - k)}.
\]
Diese Verteilung nennt man Binomialverteilung mit Parametern $n$ und $p$.
Die Unabhängigkeit ist hier sehr wichtig, da diese für das Herleiten von $f_X$ aus denen der einzelnen $f_{X_i}$ hergenommen wird.
%
%
%
%
%
\subsubsection{Kenngrößen von Zufallsgrößen}
%
Will man eine Zufallsgröße auf einige wenige Kennzahlen herunterbrechen, die in der Praxis auch relevant sein könnten, so bieten sich sogenannte Momente deren Verteilung an.
%
%
\paragraph{Erwartungswert}
Interessiert man sich für den Wert, der sich \q{im Mittel} bei einer Verteilung $X$ ergibt, so berechnet man den Erwartungswert $\E(X)$, das erste Moment, definiert durch
\begin{equation}\label{eq:random:mean}
    \E(X) = \Int{-\infty}{+\infty}{x f_X(x)}{x}.
\end{equation}
Wenn $f_X$ einer physikalischen Dichteverteilung im Raum entspräche, so würde $\E(X)$ genau den Masseschwerpunkt dieser Verteilung darstellen.
Ist beispielsweise $X \sim \Unif(a,b)$, so gilt $\E(X) = (a+b)/2$ und falls $X \sim \mathcal{N}(\mu, \sigma^2)$, so gilt $\E(X) = \mu$.

Mit dem Erwartungswert kann man Zufallsgrößen \emph{zentrieren}, indem man für $X$ die Zufallsgröße $\tilde{X} = X - \E(X)$ betrachtet. 
Dann gilt 
\[
\E(\tilde{X}) = \E(X) - \E(\E(X)) = \E(X) - \E(X) = 0
\]
Man nennt Zufallsgrößen $X$ mit $\E(X)$ \emph{mittelwertfrei}.
%
%
\paragraph{Varianz}
%
Basierend auf dem Erwartungswert, können wir auch die mittlere Streuung einer Zufallsgröße um $\E(X)$ herum betrachten, also das zweite zentrierte Moment, die \emph{Varianz},
%
\begin{equation}\label{eq:random:var}
\Var(X) 
    = \E((X - \E(X))^2) 
    = \E(\tilde{X}^2)
    = \Int{-\infty}{+\infty}{(x-\E(X))^2 f_X(x)}{x}
    = \E(X^2) - \E(X)^2.
\end{equation}
%
Die Interpretation ist, dass $\Var(X)$ die mittlere quadratische Abweichung einer Zufallsgröße von ihrem Erwartungswert beschreibt.
Es ist anzumerken, dass es Verteilungen gibt, für welche $\E$ und $\Var$ nicht existieren, weil die jeweiligen Integrale in \eqref{eq:random:mean} und \eqref{eq:random:var} nicht endlich sind.

Beispielsweise gilt im Falle von $X \sim \Unif(a,b)$, dass $\Var(X) = (b-a)^2/12$ und im Falle von $X \sim \mathcal{N}(\mu, \sigma^2)$, dass $\Var(X) = \sigma^2$.
%
%
\paragraph{Korrelation/Kovarianz} 
Es kommt vor, dass wir auf einfache Weise zwei Zufallsgrößen $X$ und $Y$ mit einander in Zusammenhang bringen wollen.
Hierzu kann man deren \emph{Korrelation} $\E(X Y)$ betrachten, also den Erwartungswert von deren Produkt $Z = X \cdot Y$.
Betrachtet man stattdessen die Korrelation der zentrierten Versionen $\E(\tilde{X} \tilde{Y})$, so spricht man von der \emph{Kovarianz}.

Es ist wieder darauf hinzuweisen, dass man Korrelation und Kovarianz nicht \q{einfach} berechnen kann, weil man hierfür wieder die gemeinsame Verteilung der beiden Zufallsgrößen benötigt.
Sind aber $X$ und $Y$ unabhängig, dann gilt
\begin{align*}
\E(X Y) 
    & = \Int{}{}{
        \Int{}{}{x y f_{x,y}(x,y) }{y}
    }{x}
    = \Int{}{}{
        \Int{}{}{x y f_{x}(x) f_{y}(y) }{y}
    }{x}
    = \Int{}{}{
        x f_x(x)\Int{}{}{y f_{y}(y) }{y}
    }{x} \\
    & = \E(Y) \Int{}{}{
        x f_x(x)\Int{}{}{y f_{y}(y) }{y}
    }{x}
    = \E(X) \E(Y).
\end{align*}
Für unabhängige Zufallsgrößen ist also der Erwartungswert des Produktes das Produkt der einzelnen Erwartungswerte. 
Eine sehr nützliche Eigenschaft, wie wir bald sehen werden!
%
%
\paragraph{Charakteristische Funktion}
Gegeben eine Zufallsgröße $X$ mit Dichte $f_X$, ist die zugehörige \emph{charakteristische Funktion} $\phi_X : \R \rightarrow \C$ definiert durch
\begin{equation}
    \phi_X(f) = \Int{-\infty}{+\infty}{f_X(x)\exp(-\jmath 2 \pi f x)}{x}.
\end{equation}
Damit ist $\phi_X$ die Fourier-Transformierte von $f_X$.
Wir werden bald den Nutzen von dieser Funktion sehen, aber es sei beispielhaft erwähnt, dass
\[
\left(\frac{\mathrm{d}}{\mathrm{d}f}\right)^k\phi_X(0) = \jmath^k \E(X^k),
\]
also die Ableitungen von $\phi_X$ mit den Momenten von $X$ in Zusammenhang stehen.
%
%
%
\subsection{Zufällige Signale}
%
Die bisherigen Konzepte reichen jedoch nicht aus, um die zeitliche Komponente von Zufallsprozessen richtig modellieren zu können, da wir bis jetzt nur einzelne Zufallsgrößen betrachtet haben, die nicht in einem gemeinsamen (beispielsweise zeitlichen) Kontext gebracht wurden.
Wenn wir von zufallsbehafteten Signalen, beispielsweise im Zeitbereich, sprechen, so stellen wir uns vor, dass an jedem Zeitpunkt $t$ eine Zufallsgröße $X_t$ \q{angeheftet} ist, die sich dann in ihrer Gesamtheit zu einer zufälligen Funktion $f : \R \rightarrow \C$ realisieren, indem wir $f(t) = X_t(\omega)$ setzen.
Die \emph{zufällige} Funktion $f$ ist also die punktweise Realisierung der verschiedenen Zufallsprozesse $X_t$, siehe \Cref{py:random:signal1}.
Man nennt dann die Folge von Zufallsgrößen $(X_t)_{t \in \R}$ einen \emph{stochastischen Prozess}, oder einfach \emph{zufälliges/stochastisches Signal}.
Eine spezielle Realisierung zu einer zufälligen Funktion $f$ nennen wir auch \emph{Pfad}.

Daraus ergeben sich nun zwei Sichtweisen auf stochastische Signale. 
Wir können von einer Folge von Zufallsgrößen im Zeitbereich sprechen, und deren gemeinsame Verteilung betrachten.
Alternativ können wir auch direkt auf den Pfaden eine andere, aber darauf bezogene, Verteilung im Raum der Funktionen betrachten. 
Im ersten Fall betrachten wir die Verteilung zu einzelnen Zeitpunkten, aber gemeinsam für alle Punkte.
Im zweiten betrachten wir das Verhalten als \emph{Ganzes}.
Beide Sichtweisen sind in \Cref{py:random:signal2} zusammengefasst.

\begin{listing}[ht]
    \noindent
    \begin{minipage}{0.51\textwidth}
        \strut\vspace*{-\baselineskip}\newline
        \inputminted[firstline=5, lastline=8]{python3}{code/random/signal1.py}
    \end{minipage}%
    \begin{minipage}{0.48\textwidth}
        \strut\vspace*{-\baselineskip}\newline
        \includegraphics[width=\textwidth]{code/random/signal1.png}
    \end{minipage}
    \codecaption{dsv/code/random/signal1.py}{Drei Realisierungen von $\sin(2 \pi t) + 3/10 X_t$, wobei alle $X_t \sim \mathcal{N}(0,1)$ unabhängig identisch verteilt sind (\gls{iid})}\label{py:random:signal1}
\end{listing}

Wie bereits in den vorherigen Abschnitten, wollen wir darauf hinweisen, dass man solche stochastischen Signale auch ohne viel Aufwand im Raum definieren kann.
Man kann also auch Bild-Signale der Form $(X_{x,y})_{x,y \in \R}$ als räumlichen Prozess auffassen/modellieren.

\begin{listing}[ht]
    \noindent
    \begin{minipage}{0.51\textwidth}
        \strut\vspace*{-\baselineskip}\newline
        \inputminted[firstline=5, lastline=19]{python3}{code/random/signal2.py}
    \end{minipage}%
    \begin{minipage}{0.48\textwidth}
        \strut\vspace*{-\baselineskip}\newline
        \includegraphics[width=\textwidth]{code/random/signal2.png}
    \end{minipage}
    \codecaption{dsv/code/random/signal2.py}{Visualisierung der beiden Sichtweisen auf stochastische Signale.}\label{py:random:signal2}
\end{listing}

Es gibt zu stochastischen Signalen eine Menge zu sagen und man kann viel Theorie dazu konsumieren und betreiben. 
Wir wollen uns jedoch direkt auf eine kleine Klasse von stochastischen Signalen beschränken.

\subsection{Diskrete Stationäre Signale}\label{sec:random:station}

Nehmen wir an, dass wir eine diskrete Folge von Zufallsgrößen $(X_{n})_{n \in \N}$, also ein diskretes stochastisches Signal gegeben haben.
Dann gibt es für diese Werte eine gemeinsame Dichte $f_{X_{1}, X_{2}, \ldots}(x_1, x_2, \ldots)$.
Wenn wir nun diese Samples um ein $\tau \in \N$ verschieben, also $(X_{n + \tau})_{n \in \N}$ verschieben und 
%
\[
f_{X_{1}, X_{2}, \ldots}(x_1, x_2, \ldots) =
    f_{X_{1 + \tau}, X_{2} + \tau, \ldots}(x_1, x_2, \ldots)
\]
%
für alle Stellen $n \in \N$ und $\tau \in \N$ gilt, dann nennt man den Prozess $X_n$ \emph{stark stationär}.
Dies führt dazu, dass sowohl Erwartungswert als auch Varianz \emph{konstant} sind, also $\E(X_n) = \E(X_0) = c$.
Umgekehrt muss der Prozess nicht stationär sein, wenn die Momente zeitlich konstant sind.

Wir definieren die \gls{acf} $\gamma_{X,X}: \N \times \N \rightarrow \R$ als
\begin{equation}
    \label{eq:random:acf}
    \gamma_{X,X}[n_1, n_2] = \E(X_{n_1} X^\ast_{n_2}).
\end{equation}
Somit stellt $\gamma$ die Korrelation der Prozesses $X_n$ zwischen verschiedenen Zeitpunkten dar.
Ist nun $X_n$ stark stationär, dann unterscheidet sich die Verteilung von $X_n$ nicht von der von $X_{n + \tau}$.
Also gilt wegen
\begin{equation}
    \label{eq:random:stat_acf}
    \gamma_{X,X}[n_1, n_2] 
        = \gamma_{X,X}[n_1 + \tau, n_2 + \tau]
        = \gamma_{X,X}[n_1 - n_2, 0] = \gamma_{X,X}[n_1 - n_2],
\end{equation}
dass die \gls{acf} nur von $n_1-n_2$ abhängig, also dem Abstand der Abtastzeitpunkte, ist.
Da wir in \eqref{eq:random:stat_acf} auch in die andere Richtung hätten argumentieren können, gilt $\gamma_{X,X}[\tau] = \gamma_{X,X}[-\tau]$, was $\gamma$ zu einer geraden Funktion macht.

Es ist nun essentiell zu sehen, dass wir mit $\gamma$ aus einem stochastischen Objekt, also dem stationären Prozess $X_n$ die \emph{deterministische}, also nicht-zufällige, Funktion $\gamma$ gewonnen haben, die uns gleichzeitig etwas über das zeitliche Verhalten des Prozesses, also des Signals, mitteilen kann.
Für stationäre stochastische Signale bildet $\gamma$ also eine Brücke zwischen der zufälligen und stochastischen Welt.

Wir sehen das noch direkter an folgendem 
\begin{Thm}[Rainbows'n'Sunshine Wiener-Chintschin]
    \label{thm:random:wiener-chintschin}
    Für einen stationären Prozess $X_n$ mit \gls{acf} $\gamma_{X,X}$ existiert eine Funktion $S: [-1/2,+1/2] \rightarrow \C$ sodass,
    \begin{equation}
        \gamma_{X,X}(n) = \Int{-1/2}{+1/2}{
            S(f) \exp(\jmath 2 \pi f n)
        }{f}
    \end{equation}
    gilt.
    Man nennt dann $S$ die spektrale Leistungsdichte von $X_n$.
\end{Thm}

\begin{listing}[ht]
    \noindent
    \begin{minipage}{0.51\textwidth}
        \strut\vspace*{-\baselineskip}\newline
        \inputminted[firstline=6, lastline=39]{python3}{code/random/signal3.py}
    \end{minipage}%
    \begin{minipage}{0.48\textwidth}
        \strut\vspace*{-\baselineskip}\newline
        \includegraphics[width=\textwidth]{code/random/signal3.png}
    \end{minipage}
    \codecaption{dsv/code/random/signal3.py}{Extraktion der spektralen Komponenten eines stationären Signals}\label{py:random:signal3}
\end{listing}

Wir zeigen in \Cref{py:random:signal3}, wie man aus mehreren Realisierungen eines stationären Signals
\[
x[n] = X_n = \sin(2 \pi 0.75 n \Delta t + \phi_1) + \sin(2 \pi 3 n \Delta t + \phi_2) + z_n,
\]
wobei $z_n \sim \mathcal{N}(0, 0.6^2)$ und $\phi_{1,2} \sim \Unif(0, 2\pi)$ \gls{iid}, die spektralen Komponenten extrahieren kann.
Von diesem zufälligen Signal haben wir in dem oberen Plot von \Cref{py:random:signal3} verschiedene Realisierungen dargstellt.
Im mittleren Plot haben wir Histogramme von $X_{10}$ und von $X_{50}$ geplottet, um eine Möglichkeit zu zeigen, wie man zumindest \emph{visuel} prüfen kann, ob das Signal $x[n]$ stationär ist.
Wir sehen, dass die Histogramme in etwa übereinstimmen, also können wir von Stationarität ausgehen, da gleiche Histogramme an verschiedenen Zeitpunkten auf eine gleiche Dichte an diesen Zeitpunkten schließen lassen.
Beispielsweise wäre das Signal aus \Cref{py:random:signal2} \emph{nicht} stationär.

Im unteren Plot zeigen wir eine Schätzung von $\gamma_{X,X}$ basiernd auf mehreren Realisierungen des Signals $x[\cdot]$, indem wir einfach für jedes einzelne $x[\cdot]$ die Faltung $x \ast x[-\cdot]$ berechnet haben und dann über \emph{diese} Realisierungen gemittelt, um den Erwartungswert in \eqref{eq:random:acf} zu approximieren.
Man sieht sehr gut, dass die (\textbf{zufällige}) Approximation von $\gamma_{X,X}$ sich sehr stabil verhält, auch wenn wir $N$ in \Cref{py:random:signal3} viel kleiner werden lassen.
Weiterhin sehen wir im Plot ganz unten den Effekt von \Cref{thm:random:wiener-chintschin}, weil die \gls{dft} von $\gamma_{X,X}$ an den Stellen $F=\pm 3$ und $F = \pm 0.75$ lokale Maxima zu sehen sind, was genau den spektralen Komponenten des \emph{zufälligen} Signals $X_n$ entspricht.

Mit der Berechnung der \gls{acf} kann man also aus einzelnen wenigen Realisierungen von zufälligen Signalen deren spektrale Komponenten extrahieren.
Diese Herangehensweise ist durch das Wiener-Chintschin-\Cref{thm:random:wiener-chintschin} theoretisch untermauert.
%
%
\subsection{Quantisierung}\label{sec:random:quanti}

% - \cite{widrow2008quantization}
% - area sampling (geoemetrisch)
% - charakteristische funktion: value at 0, conj sym, 
% - areasampling: analytisch in value domain, in frequ domain
% - reconstruction der PDF aus der quantisierten PDF
% - PQN Model

% \input{content/paramest.tex}